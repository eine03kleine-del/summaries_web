import os
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
from urllib.parse import quote, urljoin
import re
import time

# ==========================================
# è¨­å®š
# ==========================================
BASE_URL = "https://www.aozora.gr.jp/"
SAVE_DIR = "aozora_summaries"
os.makedirs(SAVE_DIR, exist_ok=True)

# ==========================================
# é–¢æ•°ç¾¤
# ==========================================

def get_author_list():
    """ä½œå®¶ä¸€è¦§ã‚’å–å¾—ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    url = BASE_URL + "index_pages/person_all.html"
    print(f"ğŸ“¥ ä½œå®¶ä¸€è¦§å–å¾—ä¸­: {url}")
    res = requests.get(url, timeout=10)
    res.encoding = res.apparent_encoding
    soup = BeautifulSoup(res.text, "html.parser")

    authors = []
    # olã‚¿ã‚°å†…ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—
    for link in soup.select("ol li a"):
        name = link.text.strip()
        href = link.get("href")
        if href:
            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
            full_url = urljoin(url, href)
            # ã‚¢ãƒ³ã‚«ãƒ¼ã‚’å‰Šé™¤
            full_url = full_url.split('#')[0]
            authors.append((name, full_url))
    
    print(f"âœ… å–å¾—ã—ãŸä½œå®¶æ•°: {len(authors)}")
    
    # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®5äººã®URLã‚’è¡¨ç¤º
    print("\nğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«URL:")
    for i, (name, url) in enumerate(authors[:5]):
        print(f"  {i+1}. {name}: {url}")
    
    return authors

def test_author_page(author_url):
    """ä½œå®¶ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒ†ã‚¹ãƒˆ"""
    try:
        res = requests.get(author_url, timeout=5)
        if res.status_code == 404:
            return False, "404 Not Found"
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "html.parser")
        
        # cardsã‚’å«ã‚€ãƒªãƒ³ã‚¯ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        cards_links = [a for a in soup.find_all("a", href=True) if "cards" in a.get("href", "")]
        return True, len(cards_links)
    except Exception as e:
        return False, str(e)

def get_works_from_author(author_url):
    """ä½œå®¶ãƒšãƒ¼ã‚¸ã‹ã‚‰ä½œå“ä¸€è¦§ã‚’å–å¾—"""
    try:
        res = requests.get(author_url, timeout=10)
        if res.status_code == 404:
            print(f"  âš ï¸ 404 Not Found")
            return []
        
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "html.parser")

        works = []
        
        # ä½œå“ãƒªã‚¹ãƒˆã‚’æ¢ã™ï¼ˆè¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œï¼‰
        for link in soup.find_all("a", href=True):
            href = link.get("href", "")
            title = link.get_text(strip=True)
            
            # cardsãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            if "cards" in href and title and len(title) > 1:
                full_url = urljoin(author_url, href)
                works.append((title, full_url))
        
        # é‡è¤‡å‰Šé™¤
        seen = set()
        unique_works = []
        for title, url in works:
            if (title, url) not in seen:
                seen.add((title, url))
                unique_works.append((title, url))
                print(f"  âœ“ {title}")
        
        return unique_works
        
    except Exception as e:
        print(f"  âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def get_text_url_from_card(card_url):
    """ã‚«ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸ã‹ã‚‰HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®URLã‚’å–å¾—"""
    try:
        res = requests.get(card_url, timeout=10)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "html.parser")
        
        # HTMLç‰ˆã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        for a in soup.find_all("a", href=True):
            href = a.get("href", "")
            if "files/" in href and ".html" in href:
                return urljoin(card_url, href)
        
        return None
    except Exception as e:
        print(f"    âš ï¸ ã‚«ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def extract_text_from_work(work_url):
    """ä½œå“URLã‹ã‚‰æœ¬æ–‡ã‚’å–å¾—"""
    try:
        # ã‚«ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸ã‹ã‚‰HTMLç‰ˆã®URLã‚’å–å¾—
        text_url = get_text_url_from_card(work_url)
        if not text_url:
            print(f"    âš ï¸ HTMLç‰ˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        # æœ¬æ–‡ãƒšãƒ¼ã‚¸ã‚’å–å¾—
        res_text = requests.get(text_url, timeout=10)
        res_text.encoding = res_text.apparent_encoding
        soup_text = BeautifulSoup(res_text.text, "html.parser")
        
        # æœ¬æ–‡ã‚’æŠ½å‡º
        main_text = soup_text.find("div", class_="main_text")
        if not main_text:
            main_text = soup_text.find("body")
        
        if not main_text:
            return None
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•´å½¢
        text = main_text.get_text(separator="\n", strip=True)
        text = re.sub(r'ã€Š.*?ã€‹', '', text)
        text = re.sub(r'ï¼»ï¼ƒ.*?ï¼½', '', text)
        text = re.sub(r'ï½œ', '', text)
        
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        text = "\n".join(lines)
        
        print(f"    âœ“ {len(text)}æ–‡å­—å–å¾—")
        return text
        
    except Exception as e:
        print(f"    âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def summarize_text(text):
    """ç°¡æ˜“è¦ç´„"""
    text = text.replace("\r", "").replace("\n", " ")
    if len(text) > 500:
        return text[:500] + "..."
    return text

def save_summary(author, title, summary):
    """è¦ç´„ã‚’HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜"""
    safe_author = quote(author, safe='')
    safe_title = quote(title, safe='')
    author_dir = os.path.join(SAVE_DIR, safe_author)
    os.makedirs(author_dir, exist_ok=True)

    file_path = os.path.join(author_dir, f"{safe_title}.html")
    
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(f"""<!DOCTYPE html>
<html lang='ja'>
<head>
    <meta charset='UTF-8'>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title} - {author}</title>
    <style>
        body {{ font-family: "æ¸¸ã‚´ã‚·ãƒƒã‚¯", "Yu Gothic", sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }}
        h1 {{ color: #333; }}
        .summary {{ line-height: 1.8; background: #f5f5f5; padding: 20px; border-radius: 5px; }}
    </style>
</head>
<body>
    <h1>{title}</h1>
    <p><strong>è‘—è€…:</strong> {author}</p>
    <div class="summary">
        <h2>å†’é ­</h2>
        <p>{summary}</p>
    </div>
    <p><a href="../index.html">â† ä¸€è¦§ã«æˆ»ã‚‹</a></p>
</body>
</html>""")
    
    return file_path

# ==========================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==========================================

def main():
    all_authors = get_author_list()
    
    # æœ‰åŠ¹ãªä½œå®¶ã‚’æ¢ã™
    print(f"\n{'='*60}")
    print("ğŸ” æœ‰åŠ¹ãªä½œå®¶ãƒšãƒ¼ã‚¸ã‚’æ¢ç´¢ä¸­...")
    print(f"{'='*60}\n")
    
    valid_authors = []
    
    # æ§˜ã€…ãªç¯„å›²ã‹ã‚‰è©¦ã™
    test_ranges = [
        (0, 20, "æœ€åˆã®20äºº"),
        (50, 70, "50-70ç•ªç›®"),
        (100, 120, "100-120ç•ªç›®"),
        (200, 220, "200-220ç•ªç›®"),
    ]
    
    for start, end, label in test_ranges:
        print(f"\nğŸ“ {label}ã‚’ç¢ºèªä¸­...")
        for i in range(start, min(end, len(all_authors))):
            author, url = all_authors[i]
            
            # çŸ­ã„é–“éš”ã§ãƒ†ã‚¹ãƒˆ
            is_valid, result = test_author_page(url)
            
            if is_valid and result > 0:
                print(f"  âœ… {author}: {result}ä½œå“ã‚ã‚Š")
                valid_authors.append((author, url))
                if len(valid_authors) >= 5:
                    break
            
            time.sleep(0.2)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
        
        if len(valid_authors) >= 5:
            break
    
    if not valid_authors:
        print("\nâŒ æœ‰åŠ¹ãªä½œå®¶ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        print("ğŸ’¡ é’ç©ºæ–‡åº«ã®URLæ§‹é€ ãŒå¤‰æ›´ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        return
    
    print(f"\n{'='*60}")
    print(f"âœ… å‡¦ç†å¯¾è±¡: {len(valid_authors)}å")
    print(f"{'='*60}\n")
    
    index_entries = []

    for author, author_url in tqdm(valid_authors, desc="ä½œå®¶å‡¦ç†ä¸­"):
        print(f"\nğŸ‘¤ {author}")
        print(f"   URL: {author_url}")
        
        works = get_works_from_author(author_url)[:3]
        
        if not works:
            continue
        
        safe_author = quote(author, safe='')
        author_page_name = safe_author + ".html"
        author_page_path = os.path.join(SAVE_DIR, author_page_name)

        author_entries = []
        for title, work_url in works:
            try:
                print(f"\n  ğŸ“š {title}")
                text = extract_text_from_work(work_url)
                if not text or len(text) < 100:
                    continue
                
                summary = summarize_text(text)
                path = save_summary(author, title, summary)
                rel_path = os.path.relpath(path, SAVE_DIR).replace("\\", "/")
                author_entries.append(f"        <li><a href='{rel_path}'>{title}</a></li>")
                
                time.sleep(0.5)
                
            except Exception as e:
                print(f"    âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")

        if author_entries:
            with open(author_page_path, "w", encoding="utf-8") as f:
                f.write(f"""<!DOCTYPE html>
<html lang='ja'>
<head>
    <meta charset='UTF-8'>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{author} - ä½œå“ä¸€è¦§</title>
    <style>
        body {{ font-family: "æ¸¸ã‚´ã‚·ãƒƒã‚¯", "Yu Gothic", sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }}
        h1 {{ color: #333; }}
        ul {{ list-style-type: none; padding: 0; }}
        li {{ margin: 10px 0; padding: 10px; background: #f5f5f5; border-radius: 5px; }}
        a {{ text-decoration: none; color: #0066cc; }}
        a:hover {{ text-decoration: underline; }}
    </style>
</head>
<body>
    <h1>{author} - ä½œå“ä¸€è¦§</h1>
    <ul>
{chr(10).join(author_entries)}
    </ul>
    <p><a href="index.html">â† ãƒˆãƒƒãƒ—ã«æˆ»ã‚‹</a></p>
</body>
</html>""")
            index_entries.append(f"        <li><a href='{author_page_name}'>{author} ({len(author_entries)}ä½œå“)</a></li>")
            print(f"\n  âœ… {len(author_entries)}ä½œå“å®Œäº†")

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒšãƒ¼ã‚¸ä½œæˆ
    index_path = os.path.join(SAVE_DIR, "index.html")
    with open(index_path, "w", encoding="utf-8") as f:
        f.write(f"""<!DOCTYPE html>
<html lang='ja'>
<head>
    <meta charset='UTF-8'>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>é’ç©ºæ–‡åº« è¦ç´„ã¾ã¨ã‚</title>
    <style>
        body {{ font-family: "æ¸¸ã‚´ã‚·ãƒƒã‚¯", "Yu Gothic", sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; background: #fafafa; }}
        h1 {{ color: #333; text-align: center; }}
        ul {{ list-style-type: none; padding: 0; }}
        li {{ margin: 10px 0; padding: 15px; background: white; border-radius: 5px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        a {{ text-decoration: none; color: #0066cc; font-size: 18px; }}
        a:hover {{ text-decoration: underline; }}
    </style>
</head>
<body>
    <h1>ğŸ“š é’ç©ºæ–‡åº« è¦ç´„ã¾ã¨ã‚</h1>
    <p style="text-align: center; color: #666;">é’ç©ºæ–‡åº«ã®ä½œå“ã‚’ã‚ã‚‰ã™ã˜ä»˜ãã§ã¾ã¨ã‚ã¾ã—ãŸ</p>
    <ul>
{chr(10).join(index_entries)}
    </ul>
</body>
</html>""")

    print(f"\n{'='*60}")
    print(f"âœ… å®Œäº†: {len(index_entries)}åã®ä½œå®¶")
    print(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {index_path}")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()